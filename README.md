# Linear and Logistic-Regression
Recreation of Linear and Logistic regression algorithms on sklearn IRIS &amp; DIGITS datasets

In Task 1, first of all, we had to create a function that generates 100 random points on the plane. After that, we had to run both pocket and linear regression algorithms to get weights. Lastly, we need to test the performance of both algorithms. This was done by finding the minimization square root error metric.
And then compared the performances of both algorithms. Linear Regression is a simple algorithm that can be implemented easily to give satisfactory results. Also, Linear regression fits linearly separable datasets almost perfectly. However, the limitation of this method is that it produces overly simplified results by assuming a linear relationship between the data. Regarding the Pocket algorithm, this algorithm keeps the best result seen. Probably therefore it is called the Pocket Learning Algorithm. So, we was basically implemented the perceptron learning algorithm with a memory which keeps the result of the iteration.

In Task 2 we imported our iris dataset to train our logistic regression and find in and out errors of the logistic algorithm which was done by dividing the iris to 80% and 20% for training and testing. The main limitation and difficulty that arose and could potentially arise is that I could stop the algorithm when the improvement ended. Then, I have plotted two graphs, wherein the first one Training and Testing error for 2000 iterations were visualized. While the second one the same graph was plotted, however with 100 as a number of iterations. This was done for a better understanding of the behaviour of both trends(Ein(g1t),  Etest(g1t) ) at small values of iterations. By plotting these two graphs, we ensured that training error is higher than testing one; the margin between errors was relatively small, so we likely did not overfit the model.

In Task 3, the main aim was to select the model that will perform best on the digits dataset.  First of all, we have imported the dataset itself by using the scikit-learn package and then conducted slight exploratory data analysis where I checked whether the data is balanced or not, and also made a visualization of digits.
We have implemented the Linear Regression and Logistic Regression from Task 1 and Task 2 and have done cross-validation(n_folds = 5, 10, 20, loocv) The main difficulty that we failed to solve was making a leave-one-out cross-validation with Logistic Regression, as it requires too much computational power and our algorithm was not optimized properly for the “loocv”.After getting the Generalization errors, we have plotted a graph with two trends of errors to show comparative analysis. 
We have implemented the GridSearchCV function based on 10-fold cross-validation. By using GridSearchCV, I tuned the regularization hyperparameter(‘alpha’ or ‘C’) of Linear Regression and Logistic Regression models. In particular, we have used Lasso and Ridge regression models, as the ordinary Linear Regression of Scikit-learn lacks the regularization hyperparameter. Overall, by applying the GridSearchCV, I obtained the best regularization parameter of all 3 models(Lasso, Ridge, Logistic) which yields the best accuracies. After all, we have plotted the validation curve for analyzing the bias-variance tradeoff(underfit/overfit).
To conclude, we can claim that Logistic regression shows significantly better accuracy and error measure on the problem like digits classification.
